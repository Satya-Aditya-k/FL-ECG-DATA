{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from flwr.common.logger import log\n",
    "from logging import INFO, DEBUG\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('features_data/features_cpsc_2018_extra.csv')\n",
    "df_2 = pd.read_csv('features_data/features_cpsc_2018.csv')\n",
    "df_3 = pd.read_csv('features_data/features_georgia.csv')\n",
    "df_4 = pd.read_csv('features_data/features_incart.csv')\n",
    "df_5 = pd.read_csv('features_data/features_ptb.csv') \n",
    "df_6 = pd.read_csv('features_data/features_ptb-xl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dfs\n",
    "df = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6], ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_i_df = pd.read_csv(r\"top_120_features.csv\")\n",
    "feature_i_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(feature_i_df[\"Feature Id\"])\n",
    "feature_names[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.groupby('label').filter(lambda x: len(x) >= 1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"TAb\" and \"IAVB\" from df_copy\n",
    "df_copy = df_copy[df_copy['label'] != \"TAb\"]\n",
    "df_copy = df_copy[df_copy['label'] != \"IAVB\"]\n",
    "\n",
    "df_copy['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train val and test sets stratified by target\n",
    "\n",
    "#X = df_copy.drop(columns=['label',\"label_abr\",\"merged_labels\",\"id\",\"id.1\"])\n",
    "X = df_copy.drop(columns=['label',\"id\"])\n",
    "X = X[feature_names[:150]]\n",
    "#scale the X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = df_copy['label']\n",
    "#convert y into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def normalize(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    return scaler.transform(X), scaler\n",
    "\n",
    "def create_data_loaders(X_train, y_train, X_val, y_val, X_test, y_test, n_clients):\n",
    "    # Create the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits=n_clients)\n",
    "\n",
    "    # Create the data loaders\n",
    "    train_loaders = []\n",
    "    val_loaders = []\n",
    "\n",
    "    for _, indices in skf.split(X_train, y_train):\n",
    "        X_subset = X_train.iloc[indices]\n",
    "        y_subset = y_train[indices]\n",
    "        X_subset, scaler = normalize(X_subset)\n",
    "        subset = TensorDataset(torch.from_numpy(X_subset).float(), torch.from_numpy(y_subset).long())\n",
    "        loader = DataLoader(subset, batch_size=32)\n",
    "        train_loaders.append(loader)\n",
    "\n",
    "    for _, indices in skf.split(X_val, y_val):\n",
    "        X_subset = X_val.iloc[indices]\n",
    "        y_subset = y_val[indices]\n",
    "        X_subset, _ = normalize(X_subset, scaler)\n",
    "        subset = TensorDataset(torch.from_numpy(X_subset).float(), torch.from_numpy(y_subset).long())\n",
    "        loader = DataLoader(subset, batch_size=32)\n",
    "        val_loaders.append(loader)\n",
    "\n",
    "    # Normalize the test data based on the training data and create a DataLoader\n",
    "    X_test, _ = normalize(X_test, scaler)\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    return train_loaders, val_loaders, test_loader\n",
    "\n",
    "n_clients = 8\n",
    "train_loaders, val_loaders,test_loader = create_data_loaders(X_train, y_train, X_val, y_val,X_test,y_test, n_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a figure\n",
    "fig, axs = plt.subplots(4,2, figsize=(14, 8))\n",
    "axs = axs.flatten()\n",
    "# Loop over each train loader\n",
    "for i, train_loader in enumerate(train_loaders):\n",
    "    # Get labels from train loader\n",
    "    labels = [label for _, label in train_loader]\n",
    "    labels = np.concatenate(labels)  # Concatenate list of tensors into a single numpy array\n",
    "\n",
    "   # Convert labels to pandas Series\n",
    "    labels_series = pd.Series(labels)\n",
    "\n",
    "    # Count occurrences of each label\n",
    "    label_counts = labels_series.value_counts().sort_index()\n",
    "\n",
    "    # Plot horizontal bar chart of label counts\n",
    "    label_counts.plot(kind='barh', ax=axs[i])\n",
    "    axs[i].set_title(f'Client {i+1}')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(150, 128)\n",
    "        self.fc2 = nn.Linear(128, 500)\n",
    "        self.fc4 = nn.Linear(500,5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        #x = nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct/total\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "    precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "    recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "    return loss, accuracy,f1, precision, recall, true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=5)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy,f1, precision, recall,true_labels, pred_labels = test(self.net, self.valloader)\n",
    "        # save_path = r\"Non-IID Dist Scaling\\8 clients\\classification_report_client_\"+str(self.cid)+\"_8_clients.csv\"\n",
    "        # # plt.savefig(save_path)\n",
    "        # # plt.show()\n",
    "        # #save the plot of confusion matrix\n",
    "        # #get classification report  and save it\n",
    "        # #get the classification report\n",
    "        # from sklearn.metrics import classification_report\n",
    "        # report = classification_report(true_labels, pred_labels,target_names=le.classes_,output_dict=True)\n",
    "        # report_df = pd.DataFrame(report).transpose()\n",
    "        # report_df.to_csv(save_path)\n",
    "        log(INFO, f\"Client side accuracy: {float(accuracy)}\")\n",
    "        return float(loss), len(self.valloader),{\"accuracy\": float(accuracy),\n",
    "                                                 \"f1\": float(f1),\n",
    "                                                 \"precision\":float(precision),\n",
    "                                                 \"recall\":float(recall)}\n",
    "    \n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = train_loaders[int(cid)]\n",
    "    valloader = val_loaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `evaluate` function will be by Flower called after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: fl.common.NDArrays,\n",
    "    config: Dict[str, fl.common.Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "    net = Net().to(DEVICE)\n",
    "    valloader = test_loader\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy,f1, precision, recall, true_labels, pred_labels = test(net, valloader)\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title(f'Confusion Matrix for Server Evaluation')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy,\n",
    "                    \"f1\":float(f1),\n",
    "                    \"precision\":float(precision),\n",
    "                    \"recall\":float(recall)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(metrics):\n",
    "    \"\"\"Aggregate metrics from multiple clients by calculating mean averages.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics (list): A list containing tuples, where each tuple represents metrics for a client.\n",
    "                    Each tuple is structured as (num_examples, metric), where:\n",
    "                    - num_examples (int): The number of examples used to compute the metrics.\n",
    "                    - metric (dict): A dictionary containing custom metrics provided as `output_dict`\n",
    "                                    in the `evaluate` method from `client.py`.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary with the aggregated metrics, calculating mean averages. The keys of the\n",
    "    dictionary represent different metrics, including:\n",
    "    - 'accuracy': Mean accuracy calculated by TensorFlow.\n",
    "    - 'acc': Mean accuracy from scikit-learn.\n",
    "    - 'rec': Mean recall from scikit-learn.\n",
    "    - 'prec': Mean precision from scikit-learn.\n",
    "    - 'f1': Mean F1 score from scikit-learn.\n",
    "\n",
    "    Note: If a weighted average is required, the `num_examples` parameter can be leveraged.\n",
    "\n",
    "    Example:\n",
    "        Example `metrics` list for two clients after the last round:\n",
    "        [(10000, {'prec': 0.108, 'acc': 0.108, 'f1': 0.108, 'accuracy': 0.1080000028014183, 'rec': 0.108}),\n",
    "        (10000, {'f1': 0.108, 'rec': 0.108, 'accuracy': 0.1080000028014183, 'prec': 0.108, 'acc': 0.108})]\n",
    "    \"\"\"\n",
    "\n",
    "    # Here num_examples are not taken into account by using _\n",
    "    accuracies_tf = np.mean([metric[\"accuracy\"] for _, metric in metrics])\n",
    "    # accuracies = np.mean([metric[\"acc\"] for _, metric in metrics])\n",
    "    recalls = np.mean([metric[\"recall\"] for _, metric in metrics])\n",
    "    precisions = np.mean([metric[\"precision\"] for _, metric in metrics])\n",
    "    f1s = np.mean([metric[\"f1\"] for _, metric in metrics])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracies_tf,\n",
    "        # \"acc\": accuracies,\n",
    "        \"rec\": recalls,\n",
    "        \"prec\": precisions,\n",
    "         \"f1\": f1s,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(Net())\n",
    "\n",
    "# Pass parameters to the Strategy for server-side parameter initialization\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.3,\n",
    "    fraction_evaluate=0.3,\n",
    "    min_fit_clients=2,\n",
    "    min_evaluate_clients=2,\n",
    "    min_available_clients=n_clients,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "    evaluate_fn = evaluate,\n",
    "    evaluate_metrics_aggregation_fn = average_metrics\n",
    ")\n",
    "\n",
    "# strategy = AggregateCustomMetricStrategy(\n",
    "#     fraction_fit=0.7,\n",
    "#     fraction_evaluate=0.3,\n",
    "#     min_fit_clients=2,\n",
    "#     min_evaluate_clients=2,\n",
    "#     min_available_clients=n_clients,\n",
    "#     initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "#     evaluate_fn = evaluate,\n",
    "# )\n",
    "\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "# Start simulation\n",
    "history_sim = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=n_clients,\n",
    "    config=fl.server.ServerConfig(num_rounds=15),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    "    ray_init_args={\"include_dashboard\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all metrics from history_sim as df csv \n",
    "\n",
    "history_distributed = pd.DataFrame(history_sim.metrics_distributed)\n",
    "history_centralized = pd.DataFrame(history_sim.metrics_centralized)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
